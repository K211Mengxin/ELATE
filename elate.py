# -*- coding: utf-8 -*-
"""gp_newton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bt0V98yIE6vSHSt6cc9TjaE108fgBWid
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
import math
import jax
import jax.numpy as jnp

import numpy as np
from scipy.optimize import minimize
from jax.scipy.special import logsumexp
from jax.scipy.linalg import expm
from jax.scipy.special import logit
import jax.random as jrandom
from jax import jacfwd, jacrev
import time
from functools import partial

import optax.tree_utils as otu
import optax
plt.style.use('ggplot')

# penalty log-lik
wei_params = {
    "w_pen_ls": 1e-11, #for least square
    "w_pen_nll":1e-11 #for negative log likelihood
}

kxy = lambda t, u, ell: jnp.exp(-0.5 * jnp.square((t-u)/ell))
dkxy = lambda t, u, ell: -(t-u) / jnp.square(ell) * kxy(t, u, ell)
ddkxy = lambda t, u, ell: (1 - jnp.square((t-u)/ell)) / jnp.square(ell) * kxy(t, u, ell)

mapped = lambda func: jax.vmap(jax.vmap(func, (None, 0, None)), (0, None, None))
mapped1 = lambda func: jax.vmap(func, (0, None, None))

def build_funcdict_derivs(xs, ys, sigma2,p_deg,q_deg):#, jitter=1e-5):

    def constraints_violation(q):
        _x = jnp.linspace(-0.1, 1.1, 4000)
        #_x = jrandom.uniform(key, )
        test=1/jnp.square(jnp.polyval(q, _x))
        result = jax.scipy.integrate.trapezoid(test, _x)
        return result

    def mean_func(p,q, x):
         def rational_function(_x, p, q):
            p_poly = jnp.polyval(p, _x)
            q_poly = jnp.polyval(q, _x)
            return p_poly / q_poly
         return jnp.array(jax.value_and_grad(rational_function, 0)(x, p, q))

    def ls_mean(p_mean):
        p=p_mean[0:p_deg+1]
        q=p_mean[p_deg+1:p_deg+q_deg+2]
        res = jnp.square(jax.vmap(mean_func, (None,None,0))(p,q, xs) - ys)
        sigma=sigma2.reshape(2,len(xs)).T
        res=res/sigma
        penalty= wei_params['w_pen_ls'] * constraints_violation(q)
        return jnp.mean(res)+ penalty

    def transform(params):
        #p_mean, logell, logv = params
        v =jax.nn.softplus(params[-1])+ 1e-4#jax.nn.sigmoid(logv) + 1e-4 #jnp.exp(logv)+1e-4
        ell = jax.nn.softplus(params[-2])
        p=params[0:p_deg+1]
        q=params[p_deg+1:p_deg+q_deg+2]
        return p,q,ell, v

    def nll(params):
        p,q,ell,v=transform(params)
        K = gram_matrix(ell, xs)
        K1 = K + jnp.diag(sigma2 / v)#+1e-6* jnp.eye(K.shape[0])

        chol = jnp.linalg.cholesky(K1)
        chol_inv = jax.scipy.linalg.solve_triangular(chol, jnp.eye(chol.shape[0]), lower=True)
        residual = jnp.transpose(ys - jax.vmap(mean_func, (None,None, 0))(p,q, xs)).reshape((-1,))
        Ly = jnp.dot(chol_inv, residual)
        logliks = -0.5 * jnp.square(Ly) / v - jnp.log(jnp.abs(jnp.diagonal(chol))) - 0.5 * jnp.log(2*jnp.pi) - 0.5 * jnp.log(v)
        penalty = wei_params['w_pen_nll'] * constraints_violation(q)
        return -jnp.mean(logliks)#+penalty

    def gram_matrix(ell, _xs):
        A = mapped(kxy)(_xs, _xs, ell)
        B = mapped(dkxy)(_xs, _xs, ell)
        C = mapped(ddkxy)(_xs, _xs, ell)
        M1 = jnp.concatenate([A, -B], -1)
        M2 = jnp.concatenate([B, C], -1)
        return jnp.concatenate([M1, M2], 0)

    def kxu(ell, xnew):
        A = mapped1(kxy)(xs, xnew, ell)
        B = mapped1(dkxy)(xs, xnew, ell)
        D = mapped1(ddkxy)(xs, xnew, ell)
        M1 = jnp.concatenate([A, B], -1)
        M2 = jnp.concatenate([-B, D], -1)
        xn = jnp.array(xnew).reshape((1,))
        kuu = gram_matrix(ell, xn)
        return jnp.stack([M1, M2], 1), kuu


    def conditional(params, xnew):
        p,q, ell, v = transform(params)
        K = gram_matrix(ell, xs) + jnp.diag(sigma2 / v)
        Kxu, Kuu = kxu(ell, xnew)

        chol = jnp.linalg.cholesky(K)
        chol_inv = jax.scipy.linalg.solve_triangular(chol, jnp.eye(chol.shape[0]), lower=True)

        residual = jnp.transpose(ys-jax.vmap(mean_func, (None,None, 0))(p,q, xs)).reshape((-1,))
        #Ly = 1/jnp.sqrt(v) * jnp.dot(chol_inv, residual)
        Ly = jnp.dot(chol_inv, residual)
        pmean = mean_func(p,q, xnew)
        pmean += Kxu.T @ chol_inv.T @ Ly
        vs = v * (Kuu - Kxu.T @ chol_inv.T @ chol_inv @ Kxu)
        return pmean, jnp.diagonal(vs)

    def p_mean_init(p_deg, q_deg):
        p=jnp.ones(p_deg+1)*0.1
        q=jnp.ones(q_deg+1)*0.1
        init_p_mean = jnp.concatenate([p,q])
        fun = jax.jit(ls_mean)

        print(
            f'Initial value: {fun(init_p_mean):.2e}, '
            f'Initial gradient norm: {otu.tree_l2_norm(jax.grad(fun)(init_p_mean)):.2e}'
        )
        J= jax.grad(fun)
        H= jax.jacobian(J)
        # J=jacrev(fun)
        # H=jax.hessian(fun)
        res = minimize(fun, init_p_mean, method='Newton-CG',
               jac=J, hess=H,
               options={'xtol': 1e-10, 'disp': True})
        p_init=res.x[0:p_deg+1]
        q_init=res.x[p_deg+1:p_deg+q_deg+2]

        print(
            f'Final value: {fun(res.x):.2e}, '
            f'Final gradient norm: {otu.tree_l2_norm(jax.grad(fun)(res.x)):.2e}'
        )
        return p_init,q_init,ls_mean(res.x)

    def params_optimize(p,q):
        res = ys - jax.vmap(mean_func, (None,None, 0))(p,q, xs)
        #initialise logell and logv
        init_params = jnp.concatenate([p,q,
                                       jnp.atleast_1d(jnp.array(0.0)),
                                       jnp.atleast_1d(jnp.log(jnp.var(res[:, 0])))])
        fun = jax.jit(nll)
        print(
            f'Initial value: {fun(init_params):.2e}, '
            f'Initial gradient norm: {otu.tree_l2_norm(jax.grad(fun)(init_params)):.2e}'
        )
        #opt = optax.lbfgs()
        J= jax.grad(fun)
        H= jax.jacobian(J)
        # J=jacrev(fun)
        # H=jax.hessian(fun)
        res = minimize(fun, init_params, method='Newton-CG',
               jac=J, hess=H,
               options={'xtol': 1e-10, 'disp': True})
        params=res.x
        print(
        f'Final value: {fun(params):.2e}, '
        f'Final gradient norm: {otu.tree_l2_norm(jax.grad(fun)(params)):.2e}'
    )

        return init_params,params,nll(params)

    func_dict = {}
    func_dict['p_mean_init'] = p_mean_init
    func_dict['params_optimize'] = params_optimize
    func_dict['conditional'] = conditional
    func_dict['mean'] = mean_func
    func_dict['nll'] = nll
    func_dict['gram']= gram_matrix
    func_dict['ls_mean'] = ls_mean
    func_dict['trans']=transform
    return func_dict


#input array for temperature list, sample mean, derivative, and corresponding variance
def run_gp(ts,ys,ys_dev,var_g,var_g_dev):

    # for parameter alpha
    list_names=['post_mean', 'post_var', 'prior_fitted','p','q', 'ell',
                  'v', 'p_init', 'q_init', 'ell_init', 'v_init', 'smc_g1',
                  'smc_sigma', 'nll','mse']
    data={name:[] for name in list_names}
    sigma2 = jnp.concatenate([var_g,var_g_dev], -1)
    ys = jnp.stack([ys, ys_dev], -1)
    #training gp
    def opt(ts, ys, sigma2):
        deg=[1,2]
        nll_opt=1e+10
        for i in range(len(deg)):
            for j in range(len(deg)):
                func_dict = build_funcdict_derivs(ts, ys, sigma2,deg[i],deg[j])#, jitter=1e-4)
                p_init,q_init,mse = func_dict['p_mean_init'](deg[i], deg[j])
                init_params,params,nll= func_dict['params_optimize'](p_init,q_init)
                print(init_params)
                print(nll)
                if nll<nll_opt:
                    nll_opt=nll
                    print(init_params)
                    init_param_opt=init_params
                    param_opt=params
                    deg_num=deg[i]
                    deg_denom=deg[j]
                    
        return nll_opt,init_param_opt,param_opt,deg_num,deg_denom

    nll,init_params,params,deg_num,deg_denom=opt(ts,ys,sigma2)
    print('selected degree:',deg_num,deg_denom)
    func_dict = build_funcdict_derivs(ts, ys, sigma2,deg_num,deg_denom)
    p,q,ell,v=func_dict['trans'](params)
    p_init,q_init,ell_init,v_init=func_dict['trans'](init_params)
    #making prediction
    t0 = jnp.linspace(0, 1, 500)
    post_mean, post_var = jax.vmap(func_dict['conditional'], (None, 0))(params, t0)
    # pass the coefficient list to the mean function
    prior_fitted = jax.vmap(func_dict['mean'], (None,None,0))(p,q, t0)

    data['post_mean']=post_mean
    data['post_var']=post_var
    data['prior_fitted']=prior_fitted
    data['p']=p
    data['q']=q
    data['ell']=ell
    data['v']=v
    data['p_init']=p_init
    data['q_init']=q_init
    data['ell_init']=ell_init
    data['v_init']=v_init
    data['nll']=nll

    return data


def build_funcdict(xs, ys, sigma2,p_deg,q_deg):#, jitter=1e-5):

    def constraints_violation(q):
        _x = jnp.linspace(-0.1, 1.1, 4000)
        #_x = jrandom.uniform(key, )
        test=1/jnp.square(jnp.polyval(q, _x))
        result = jax.scipy.integrate.trapezoid(test, _x)
        return result

    def mean_func(p,q, x):
         def rational_function(_x, p, q):
            p_poly = jnp.polyval(p, _x)
            q_poly = jnp.polyval(q, _x)
            return p_poly / q_poly
         return jnp.array(rational_function(x, p, q))

    def ls_mean(p_mean):
        p=p_mean[0:p_deg+1]
        q=p_mean[p_deg+1:p_deg+q_deg+2]
        res = jnp.square(jax.vmap(mean_func, (None,None,0))(p,q, xs) - ys)
        sigma=sigma2
        res=res/sigma
        penalty= wei_params['w_pen_ls'] * constraints_violation(q)
        return jnp.mean(res)+ penalty

    def transform(params):
        #p_mean, logell, logv = params
        v =jax.nn.softplus(params[-1])+ 1e-4#jax.nn.sigmoid(logv) + 1e-4 #jnp.exp(logv)+1e-4
        ell = jax.nn.softplus(params[-2])
        p=params[0:p_deg+1]
        q=params[p_deg+1:p_deg+q_deg+2]
        return p,q,ell, v

    def nll(params):
        p,q,ell,v=transform(params)
        K = gram_matrix(ell, xs)
        K1 = K + jnp.diag(sigma2 / v)#+1e-6* jnp.eye(K.shape[0])

        chol = jnp.linalg.cholesky(K1)
        chol_inv = jax.scipy.linalg.solve_triangular(chol, jnp.eye(chol.shape[0]), lower=True)
        residual = jnp.transpose(ys - jax.vmap(mean_func, (None,None, 0))(p,q, xs)).reshape((-1,))
        Ly = jnp.dot(chol_inv, residual)
        logliks = -0.5 * jnp.square(Ly) / v - jnp.log(jnp.abs(jnp.diagonal(chol))) - 0.5 * jnp.log(2*jnp.pi) - 0.5 * jnp.log(v)
        penalty = wei_params['w_pen_nll'] * constraints_violation(q)
        return -jnp.mean(logliks)#+penalty

    def gram_matrix(ell, _xs):
        A = mapped(kxy)(_xs, _xs, ell)
        return A
    
    def kxu(ell, xnew):
        A = mapped1(kxy)(xs, xnew, ell)  # shape: (n_train, 1)
        xn = jnp.array(xnew).reshape((1,))
        kuu = gram_matrix(ell, xn)       # shape: (1, 1)
        return A, kuu



    def conditional(params, xnew):
        p,q, ell, v = transform(params)
        K = gram_matrix(ell, xs) + jnp.diag(sigma2 / v)
        Kxu, Kuu = kxu(ell, xnew)

        chol = jnp.linalg.cholesky(K)
        chol_inv = jax.scipy.linalg.solve_triangular(chol, jnp.eye(chol.shape[0]), lower=True)

        residual = jnp.transpose(ys-jax.vmap(mean_func, (None,None, 0))(p,q, xs)).reshape((-1,))
        #Ly = 1/jnp.sqrt(v) * jnp.dot(chol_inv, residual)
        Ly = jnp.dot(chol_inv, residual)
        pmean = mean_func(p,q, xnew)
        pmean += Kxu.T @ chol_inv.T @ Ly
        vs = v * (Kuu - Kxu.T @ chol_inv.T @ chol_inv @ Kxu)
        return pmean, jnp.diagonal(vs)

    def p_mean_init(p_deg, q_deg):
        p=jnp.ones(p_deg+1)*0.1
        q=jnp.ones(q_deg+1)*0.1
        init_p_mean = jnp.concatenate([p,q])
        fun = jax.jit(ls_mean)

        print(
            f'Initial value: {fun(init_p_mean):.2e}, '
            f'Initial gradient norm: {otu.tree_l2_norm(jax.grad(fun)(init_p_mean)):.2e}'
        )
        J= jax.grad(fun)
        H= jax.jacobian(J)
        # J=jacrev(fun)
        # H=jax.hessian(fun)
        res = minimize(fun, init_p_mean, method='Newton-CG',
               jac=J, hess=H,
               options={'xtol': 1e-10, 'disp': True})
        p_init=res.x[0:p_deg+1]
        q_init=res.x[p_deg+1:p_deg+q_deg+2]

        print(
            f'Final value: {fun(res.x):.2e}, '
            f'Final gradient norm: {otu.tree_l2_norm(jax.grad(fun)(res.x)):.2e}'
        )
        return p_init,q_init,ls_mean(res.x)

    def params_optimize(p,q):
        res = ys - jax.vmap(mean_func, (None,None, 0))(p,q, xs)
        #initialise logell and logv
        init_params = jnp.concatenate([p,q,
                                       jnp.atleast_1d(jnp.array(0.0)),
                                       jnp.atleast_1d(jnp.log(jnp.var(res)))])
        fun = jax.jit(nll)
        print(
            f'Initial value: {fun(init_params):.2e}, '
            f'Initial gradient norm: {otu.tree_l2_norm(jax.grad(fun)(init_params)):.2e}'
        )
        #opt = optax.lbfgs()
        J= jax.grad(fun)
        H= jax.jacobian(J)
        # J=jacrev(fun)
        # H=jax.hessian(fun)
        res = minimize(fun, init_params, method='Newton-CG',
               jac=J, hess=H,
               options={'xtol': 1e-10, 'disp': True})
        params=res.x
        print(
        f'Final value: {fun(params):.2e}, '
        f'Final gradient norm: {otu.tree_l2_norm(jax.grad(fun)(params)):.2e}'
    )

        return init_params,params,nll(params)

    func_dict = {}
    func_dict['p_mean_init'] = p_mean_init
    func_dict['params_optimize'] = params_optimize
    func_dict['conditional'] = conditional
    func_dict['mean'] = mean_func
    func_dict['nll'] = nll
    func_dict['gram']= gram_matrix
    func_dict['ls_mean'] = ls_mean
    func_dict['trans']=transform
    return func_dict


def run_gp_2(ts,ys,var_g):

    # for parameter alpha
    list_names=['post_mean', 'post_var', 'prior_fitted','p','q', 'ell',
                  'v', 'p_init', 'q_init', 'ell_init', 'v_init', 'smc_g1',
                  'smc_sigma', 'nll','mse']
    data={name:[] for name in list_names}
    sigma2 = jnp.array(var_g)
    ys = jnp.array(ys)
    def opt(ts, ys, sigma2):
        deg=[1,2]
        nll_opt=1e+10
        for i in range(len(deg)):
            for j in range(len(deg)):
                func_dict = build_funcdict(ts, ys, sigma2,deg[i],deg[j])#, jitter=1e-4)
                p_init,q_init,mse = func_dict['p_mean_init'](deg[i], deg[j])
                init_params,params,nll= func_dict['params_optimize'](p_init,q_init)
                print(init_params)
                print(nll)
                if nll<nll_opt:
                    nll_opt=nll
                    print(init_params)
                    init_param_opt=init_params
                    param_opt=params
                    deg_num=deg[i]
                    deg_denom=deg[j]
                    
        return nll_opt,init_param_opt,param_opt,deg_num,deg_denom

    nll,init_params,params,deg_num,deg_denom=opt(ts,ys,sigma2)
    func_dict = build_funcdict(ts, ys, sigma2,deg_num,deg_denom)
    #training gp
    p,q,ell,v=func_dict['trans'](params)
    p_init,q_init,ell_init,v_init=func_dict['trans'](init_params)
    #making prediction
    t0 = jnp.linspace(0, 1, 500)
    post_mean, post_var = jax.vmap(func_dict['conditional'], (None, 0))(params, t0)
    # pass the coefficient list to the mean function
    prior_fitted = jax.vmap(func_dict['mean'], (None,None,0))(p,q, t0)

    data['post_mean']=post_mean
    data['post_var']=post_var
    data['prior_fitted']=prior_fitted
    data['p']=p
    data['q']=q
    data['ell']=ell
    data['v']=v
    data['p_init']=p_init
    data['q_init']=q_init
    data['ell_init']=ell_init
    data['v_init']=v_init
    data['nll']=nll
    
    return data


